{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729ac4cf",
   "metadata": {},
   "source": [
    "# 빅분기Part5-작업2유형-분석연습\n",
    "p.389~ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72dc83",
   "metadata": {},
   "source": [
    "### 문제1\n",
    "고객 3500명에 대한 학습용 데이터(x_train, y_train)을 이용하여 성별 예측 모형을 만든 후, 이를 평가용 데이터(x_test)에 적용하여 얻은 2482명 고객의 성별 예측값(남자일 확률)을 다음과 같은 형식의 csv 파일로 생성하시오 (제출한 모델의 성능은 ROC-AUC 평가지표에 따라 채점)\n",
    "- y_train: 고객의 성별 데이터. 학습 데이터. 3500명\n",
    "- x_train, x_test: 고객의 상품 구매 속성. 학습 및 평가용\n",
    "\n",
    "##### 제출 형식\n",
    "- custid, gender\n",
    "- 3500, 0.267\n",
    "- 3501, 0.578\n",
    "- 3502, 0.885"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84310575",
   "metadata": {},
   "source": [
    "- 22.6.19.일 22:22 데이터 로드만.\n",
    "- 22.6.20.월 14:30~16:57 predict 에서 Memory부족 에러나고 막힘. 8GB 넘게 먹어서 이 놋북에서 실행불가. 혹은 램 덜 먹게 조치??\n",
    "> n_estimators= 10 으로 해결\n",
    "- 22.6.20.월 18:00~22:30 ValueError: could not broadcast input array from shape (3500,3500) into shape (3500,) 및 #ValueError: multiclass-multioutput format is not supported\n",
    "> y_test_proba 의 형태가 (2,3500,3500) 꼴로 나온다. 왜지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29572b82",
   "metadata": {},
   "source": [
    "y_train에 index가 포함된 경우\n",
    "#### predict 는 슬라이싱 하지 말아야 하고\n",
    "- model.fit(x_train, y_train) #display(pd.DataFrame(y_train_predict).head()) # <<< 2열 (인덱스,예측 결과값)\n",
    "- y_test_predict= model.predict(x_test) \n",
    "\n",
    "#### proba 는 슬라이싱 해서 gender 만 내도록 해야 한다.\n",
    "- model.fit(x_train, y_train.iloc[:,1]) # <<< 없는 cust_id 가 방해가 되나 싶어서 1열만 내도록 학습\n",
    "- y_test_proba= model.predict_proba(x_test) # <<< train 으로 만든 모델에 x_test 넣기. 결과 제출용으로 필수\n",
    "\n",
    "#### 애초에 y_train 에서 index 부분은 drop 하고 따로 저장했어야?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714c781",
   "metadata": {},
   "source": [
    "##### error 회피하려다 초가삼간 태운다\n",
    "그렇게 추정한 각 정보는 누구 id 것인데?\n",
    "> id 붙이면 해결!. train 으로 model 만들적부터 오름차순으로만 되어있으면 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c1a4f",
   "metadata": {},
   "source": [
    "### 주석 싹싹 지우고 다시2\n",
    "22.6.21 15:44/ y_train 에서 id 빼고 학습시켜야 한다. concat 은 Series 끼리만 []로 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97dcdfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델링전까지 수행시간: 0.214001 sec\n",
      "모델링 시간: 0.260998 sec\n"
     ]
    }
   ],
   "source": [
    "## 데이터 로드, null, dtypes 체크 #이상치 체크, 인코딩, 파생변수, 상관관계, 변수 드랍, x_test에도 전처리 \n",
    "## 스케일링 # 모델링, train score, test score # id붙이기, to_csv\n",
    "\n",
    "## gender 일 확률. proba\n",
    "import time\n",
    "start_time= time.time()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "x_train= pd.read_csv('bigData/x_train.csv', encoding='CP949')\n",
    "y_train= pd.read_csv('bigData/y_train.csv', encoding='CP949')\n",
    "x_test= pd.read_csv('bigData/x_test.csv', encoding='CP949') # Wow! x_test 넣어야할 것을 x_train 넣었었다.\n",
    "\n",
    "## null 처리. 환불 null 은 환불이 없었다고 가정. fillna(0)\n",
    "x_train['환불금액']= x_train['환불금액'].fillna(0)\n",
    "x_test['환불금액']= x_test['환불금액'].fillna(0)\n",
    "\n",
    "## dtypes 체크 - 숫자인데 문자로 되어있는 건 아니 보인다\n",
    "\n",
    "## 이상치 보기. 구매액이 매우 크다고 해서 절삭하는 건 말이 안 된다고 본다.\n",
    "def outlierCheck(data):\n",
    "    dataNum= data.loc[:,data.dtypes!=object] # 숫자만\n",
    "    desc= dataNum.describe()\n",
    "    min1= desc.loc['min']\n",
    "    max1= desc.loc['max']\n",
    "    std= desc.loc['std']\n",
    "    mean= desc.loc['mean']\n",
    "    maxBoundary= mean+1.5*std\n",
    "    minBoundary= mean-1.5*std\n",
    "    \n",
    "    return minBoundary, maxBoundary\n",
    "\n",
    "minB, maxB= outlierCheck(x_train)\n",
    "minB_test, maxB_test= outlierCheck(x_test)\n",
    "\n",
    "## 파생변수. 주말방문 경험 유무? 1회 최대구매액 구간- boundary 밖 & median? 환불여부?\n",
    "conditionWeekend= x_train['주말방문비율']>0\n",
    "x_train.loc[conditionWeekend, '주말방문여부']= 1\n",
    "x_train.loc[~conditionWeekend, '주말방문여부']= 0\n",
    "\n",
    "conditionRefund= x_train['환불금액']>0\n",
    "x_train.loc[conditionRefund, '환불여부']= 1\n",
    "x_train.loc[~conditionRefund, '환불여부']= 0\n",
    "\n",
    "conditionMaxBuyOutlier= x_train['최대구매액']> maxB['최대구매액']\n",
    "conditionMaxBuy= (x_train['최대구매액']<= maxB['최대구매액']) & (x_train['최대구매액'] > x_train['최대구매액'].median())\n",
    "conditionMaxBuySmall= x_train['최대구매액'] <= x_train['최대구매액'].median()\n",
    "x_train.loc[conditionMaxBuyOutlier, '최대구매액많은편']=2\n",
    "x_train.loc[conditionMaxBuy, '최대구매액많은편']=1\n",
    "x_train.loc[conditionMaxBuySmall, '최대구매액많은편']=0\n",
    "\n",
    "## 파생변수- x_test\n",
    "conditionWeekend= x_test['주말방문비율']>0\n",
    "x_test.loc[conditionWeekend, '주말방문여부']= 1\n",
    "x_test.loc[~conditionWeekend, '주말방문여부']= 0\n",
    "\n",
    "conditionRefund= x_test['환불금액']>0\n",
    "x_test.loc[conditionRefund, '환불여부']= 1\n",
    "x_test.loc[~conditionRefund, '환불여부']= 0\n",
    "\n",
    "conditionMaxBuyOutlier= x_test['최대구매액']> maxB_test['최대구매액']\n",
    "conditionMaxBuy= (x_test['최대구매액']<= maxB_test['최대구매액']) & (x_test['최대구매액'] > x_test['최대구매액'].median())\n",
    "conditionMaxBuySmall= x_test['최대구매액'] <= x_test['최대구매액'].median()\n",
    "x_test.loc[conditionMaxBuyOutlier, '최대구매액많은편']=2\n",
    "x_test.loc[conditionMaxBuy, '최대구매액많은편']=1\n",
    "x_test.loc[conditionMaxBuySmall, '최대구매액많은편']=0\n",
    "\n",
    "## 메모리 관리. 변수 지우기\n",
    "x_train.drop(columns='환불금액', inplace=True)\n",
    "x_test.drop(columns='환불금액', inplace=True)\n",
    "\n",
    "## 책- 상관관계\n",
    "x_train.drop(columns='최대구매액', inplace=True)\n",
    "x_test.drop(columns='최대구매액', inplace=True)\n",
    "\n",
    "## 인코딩. 범주화\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder= LabelEncoder()\n",
    "x_train['주구매상품']= encoder.fit_transform(x_train['주구매상품']) # astype 바로 못 붙인다\n",
    "x_train['주구매지점']= encoder.fit_transform(x_train['주구매지점'])\n",
    "x_train['주구매상품']= x_train['주구매상품'].astype('category') # inplace 없다\n",
    "x_train['주구매지점']= x_train['주구매지점'].astype('category')\n",
    "x_train['주말방문여부']= x_train['주말방문여부'].astype('category')\n",
    "x_train['환불여부']= x_train['환불여부'].astype('category')\n",
    "x_train['최대구매액많은편']= x_train['최대구매액많은편'].astype('category')\n",
    "\n",
    "## 인코딩. 범주화- x_test\n",
    "x_test['주구매상품']= encoder.fit_transform(x_test['주구매상품'])\n",
    "x_test['주구매지점']= encoder.fit_transform(x_test['주구매지점'])\n",
    "x_test['주구매상품']= x_test['주구매상품'].astype('category')\n",
    "x_test['주구매지점']= x_test['주구매지점'].astype('category')\n",
    "x_test['주말방문여부']= x_test['주말방문여부'].astype('category')\n",
    "x_test['환불여부']= x_test['환불여부'].astype('category')\n",
    "x_test['최대구매액많은편']= x_test['최대구매액많은편'].astype('category')\n",
    "\n",
    "\n",
    "## 그냥 drop 시키고 cust_id 만 따로 저장\n",
    "x_train_custid= x_train['cust_id']\n",
    "x_train.drop(columns='cust_id', inplace=True)\n",
    "\n",
    "x_test_custid= x_test['cust_id']\n",
    "x_test.drop(columns='cust_id', inplace=True)\n",
    "\n",
    "## 스케일링 - 시험때는 조건말고 눈으로 보고 컬럼 골라내서 for 문 돌리자\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "x_trainNumCols= x_train.columns[(x_train.dtypes!=object) &(x_train.dtypes!='category')]\n",
    "bigNumCols= x_trainNumCols[x_train.describe().loc['max']> 30] # 30 근거: 범주컬럼 냅두고 싶어서 눈으로 정한 값\n",
    "for col in bigNumCols:\n",
    "    x_train[col]= scaler.fit_transform(x_train[[col]])\n",
    "    \n",
    "## 스케일링 - x_test\n",
    "x_testNumCols= x_test.columns[(x_test.dtypes!=object) &(x_test.dtypes!='category')]\n",
    "bigNumCols= x_testNumCols[x_train.describe().loc['max']> 30] # 30 근거: 범주컬럼 냅두고 싶어서 눈으로 정한 값\n",
    "for col in bigNumCols:\n",
    "    x_test[col]= scaler.fit_transform(x_test[[col]])\n",
    "    \n",
    "beforeModeling= time.time()\n",
    "print(f\"모델링전까지 수행시간: {beforeModeling-start_time:.6} sec\")\n",
    "\n",
    "## 모델링\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## 메모리 이슈 -  여분이 7.9GB 인데 랜포 40으로 돌리면 8.5GB 를 드신다. 랜포말고 DecisionTree는?\n",
    "model= RandomForestClassifier(n_estimators= 10) # help(model) #fit # 메모리 줄이기. n_estimators=100 default. 10으로 해결.\n",
    "\n",
    "\n",
    "## id 가 포함된 채로 y_train 목표로 학습(fit)시킬 경우, y_test_predict 의 id 에는 중복값이 들어가있다. y_test_proba는 (2,2438,3500)이 되고.\n",
    "model.fit(x_train, y_train.iloc[:,1]) # y_test_predict 용 모델도 gender 만 넣고\n",
    "y_test_predict= model.predict(x_test) \n",
    "y_test_proba= model.predict_proba(x_test) # <<< 결과 제출용\n",
    "\n",
    "\n",
    "## help(pd.concat) # pd.concat([series1, series2], ignore_index=True, axis=1) # y_test_predict 는 ndarray\n",
    "# display(pd.concat([x_test_custid, pd.Series(y_test_predict)], ignore_index=True, axis=1)) \n",
    "\n",
    "## x_test_predict 와 x_test_proba 를 비교하여, proba 의 어느 열을 남길지 결정\n",
    "## > predict 값이 1(남자) 인 게, proba의 2번째 컬럼[1]\n",
    "## 데이터 설명에서 gender==1 이 남자.\n",
    "\n",
    "# print(f\"y_test_proba {y_test_proba} \\ny_test_proba[0] {y_test_proba[0]}\")\n",
    "# display(pd.DataFrame(y_test_proba).head()) # <<< columns=[0,1], 첫 행= [0.5, 0.5]\n",
    "# display(pd.DataFrame(y_test_proba).info())\n",
    "# display(pd.concat([ x_test_custid, pd.DataFrame(y_test_proba).iloc[:,1] ], axis=1).rename(columns={\"cust_id\":\"custid\", 1:\"gender\"}))\n",
    "genderManProba= pd.concat([ x_test_custid, pd.DataFrame(y_test_proba).iloc[:,1] ], axis=1).rename(columns={\"cust_id\":\"custid\", 1:\"gender\"})\n",
    "# print(y_train)\n",
    "\n",
    "# help(pd.DataFrame.to_csv) # index\n",
    "genderManProba.to_csv('data/김형준-1번-proba.csv', index=False)\n",
    "\n",
    "## score 는 필수사항은 아니므로 일단 보류\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "afterModeling= time.time()\n",
    "print(f\"모델링 시간: {afterModeling-beforeModeling:.6} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7b44c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(pd.concat) # pd.concat([s1, s2], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f5aaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e190aa",
   "metadata": {},
   "source": [
    "### 주석 싹 지우고 다시\n",
    "22.6.20 21:00 / x_test 에도 전처리 적용해야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f0ce636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델링전까지 수행시간: 0.181998 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>5977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>5978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>5979</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>5980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>5981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1\n",
       "0     3500  1\n",
       "1     3501  0\n",
       "2     3502  0\n",
       "3     3503  0\n",
       "4     3504  0\n",
       "...    ... ..\n",
       "2477  5977  1\n",
       "2478  5978  0\n",
       "2479  5979  0\n",
       "2480  5980  0\n",
       "2481  5981  1\n",
       "\n",
       "[2482 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_proba [[0.6 0.4]\n",
      " [0.2 0.8]\n",
      " [0.4 0.6]\n",
      " ...\n",
      " [0.4 0.6]\n",
      " [0.6 0.4]\n",
      " [0.8 0.2]] \n",
      "y_test_proba[0] [0.6 0.4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  0.6  0.4\n",
       "1  0.2  0.8\n",
       "2  0.4  0.6\n",
       "3  0.6  0.4\n",
       "4  0.5  0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2482 entries, 0 to 2481\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       2482 non-null   float64\n",
      " 1   1       2482 non-null   float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 38.9 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 데이터 로드, null, dtypes 체크 #이상치 체크, 인코딩, 파생변수 # 스케일링 # 모델링, train score, test score # 형식에 맞게 출력\n",
    "\n",
    "## gender 일 확률. proba\n",
    "import time\n",
    "start_time= time.time()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "x_train= pd.read_csv('bigData/x_train.csv', encoding='CP949')\n",
    "y_train= pd.read_csv('bigData/y_train.csv', encoding='CP949')\n",
    "x_test= pd.read_csv('bigData/x_test.csv', encoding='CP949') # Wow! x_test 넣어야할 것을 x_train 넣었었다.\n",
    "\n",
    "## null 처리. 환불 null 은 환불이 없었다고 가정. fillna(0)\n",
    "x_train['환불금액']= x_train['환불금액'].fillna(0)\n",
    "x_test['환불금액']= x_test['환불금액'].fillna(0)\n",
    "\n",
    "## dtypes 체크 - 숫자인데 문자로 되어있는 건 아니 보인다\n",
    "\n",
    "## 이상치 보기. 구매액이 매우 크다고 해서 절삭하는 건 말이 안 된다고 본다.\n",
    "def outlierCheck(data):\n",
    "    dataNum= data.loc[:,data.dtypes!=object] # 숫자만\n",
    "    desc= dataNum.describe()\n",
    "    min1= desc.loc['min']\n",
    "    max1= desc.loc['max']\n",
    "    std= desc.loc['std']\n",
    "    mean= desc.loc['mean']\n",
    "    maxBoundary= mean+1.5*std\n",
    "    minBoundary= mean-1.5*std\n",
    "    \n",
    "    return minBoundary, maxBoundary\n",
    "\n",
    "minB, maxB= outlierCheck(x_train)\n",
    "minB_test, maxB_test= outlierCheck(x_test)\n",
    "\n",
    "## 파생변수. 주말방문 경험 유무? 1회 최대구매액 구간- boundary 밖 & median? 환불여부?\n",
    "conditionWeekend= x_train['주말방문비율']>0\n",
    "x_train.loc[conditionWeekend, '주말방문여부']= 1\n",
    "x_train.loc[~conditionWeekend, '주말방문여부']= 0\n",
    "\n",
    "conditionRefund= x_train['환불금액']>0\n",
    "x_train.loc[conditionRefund, '환불여부']= 1\n",
    "x_train.loc[~conditionRefund, '환불여부']= 0\n",
    "\n",
    "conditionMaxBuyOutlier= x_train['최대구매액']> maxB['최대구매액']\n",
    "conditionMaxBuy= (x_train['최대구매액']<= maxB['최대구매액']) & (x_train['최대구매액'] > x_train['최대구매액'].median())\n",
    "conditionMaxBuySmall= x_train['최대구매액'] <= x_train['최대구매액'].median()\n",
    "x_train.loc[conditionMaxBuyOutlier, '최대구매액많은편']=2\n",
    "x_train.loc[conditionMaxBuy, '최대구매액많은편']=1\n",
    "x_train.loc[conditionMaxBuySmall, '최대구매액많은편']=0\n",
    "\n",
    "## 파생변수- x_test\n",
    "conditionWeekend= x_test['주말방문비율']>0\n",
    "x_test.loc[conditionWeekend, '주말방문여부']= 1\n",
    "x_test.loc[~conditionWeekend, '주말방문여부']= 0\n",
    "\n",
    "conditionRefund= x_test['환불금액']>0\n",
    "x_test.loc[conditionRefund, '환불여부']= 1\n",
    "x_test.loc[~conditionRefund, '환불여부']= 0\n",
    "\n",
    "conditionMaxBuyOutlier= x_test['최대구매액']> maxB_test['최대구매액']\n",
    "conditionMaxBuy= (x_test['최대구매액']<= maxB_test['최대구매액']) & (x_test['최대구매액'] > x_test['최대구매액'].median())\n",
    "conditionMaxBuySmall= x_test['최대구매액'] <= x_test['최대구매액'].median()\n",
    "x_test.loc[conditionMaxBuyOutlier, '최대구매액많은편']=2\n",
    "x_test.loc[conditionMaxBuy, '최대구매액많은편']=1\n",
    "x_test.loc[conditionMaxBuySmall, '최대구매액많은편']=0\n",
    "\n",
    "## 메모리 관리. 변수 지우기\n",
    "x_train.drop(columns='환불금액', inplace=True)\n",
    "x_test.drop(columns='환불금액', inplace=True)\n",
    "\n",
    "## 책- 상관관계\n",
    "x_train.drop(columns='최대구매액', inplace=True)\n",
    "x_test.drop(columns='최대구매액', inplace=True)\n",
    "\n",
    "## 인코딩. 범주화\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder= LabelEncoder()\n",
    "x_train['주구매상품']= encoder.fit_transform(x_train['주구매상품']) # astype 바로 못 붙인다\n",
    "x_train['주구매지점']= encoder.fit_transform(x_train['주구매지점'])\n",
    "x_train['주구매상품']= x_train['주구매상품'].astype('category') # inplace 없다\n",
    "x_train['주구매지점']= x_train['주구매지점'].astype('category')\n",
    "x_train['주말방문여부']= x_train['주말방문여부'].astype('category')\n",
    "x_train['환불여부']= x_train['환불여부'].astype('category')\n",
    "x_train['최대구매액많은편']= x_train['최대구매액많은편'].astype('category')\n",
    "\n",
    "## 인코딩. 범주화- x_test\n",
    "x_test['주구매상품']= encoder.fit_transform(x_test['주구매상품'])\n",
    "x_test['주구매지점']= encoder.fit_transform(x_test['주구매지점'])\n",
    "x_test['주구매상품']= x_test['주구매상품'].astype('category')\n",
    "x_test['주구매지점']= x_test['주구매지점'].astype('category')\n",
    "x_test['주말방문여부']= x_test['주말방문여부'].astype('category')\n",
    "x_test['환불여부']= x_test['환불여부'].astype('category')\n",
    "x_test['최대구매액많은편']= x_test['최대구매액많은편'].astype('category')\n",
    "\n",
    "\n",
    "## 그냥 drop 시키고 cust_id 만 따로 저장( 'cust_id 를 index 로 만들' - 이 방법이 좀 문제를 일으키는 것 같아서 )\n",
    "# train 에 index 떼고 y_train 을 id 째 넣어도 여전히 안된다\n",
    "# 나중에 되돌리는 건 df.reset_index(drop=False, inplace=True)\n",
    "x_train_custid= x_train['cust_id'] # x_train.index= x_train['cust_id']\n",
    "x_train.drop(columns='cust_id', inplace=True)\n",
    "\n",
    "x_test_custid= x_test['cust_id'] #x_test.index= x_test['cust_id']\n",
    "x_test.drop(columns='cust_id', inplace=True)\n",
    "\n",
    "## 스케일링 - 시험때는 조건말고 눈으로 보고 컬럼 골라내서 for 문 돌리자\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "x_trainNumCols= x_train.columns[(x_train.dtypes!=object) &(x_train.dtypes!='category')]\n",
    "bigNumCols= x_trainNumCols[x_train.describe().loc['max']> 30] # 30 근거: 범주컬럼 냅두고 싶어서 눈으로 정한 값\n",
    "for col in bigNumCols:\n",
    "    x_train[col]= scaler.fit_transform(x_train[[col]])\n",
    "    \n",
    "## 스케일링 - x_test\n",
    "x_testNumCols= x_test.columns[(x_test.dtypes!=object) &(x_test.dtypes!='category')]\n",
    "bigNumCols= x_testNumCols[x_train.describe().loc['max']> 30] # 30 근거: 범주컬럼 냅두고 싶어서 눈으로 정한 값\n",
    "for col in bigNumCols:\n",
    "    x_test[col]= scaler.fit_transform(x_test[[col]])\n",
    "    \n",
    "beforeModeling= time.time()\n",
    "print(f\"모델링전까지 수행시간: {beforeModeling-start_time:.6} sec\")\n",
    "\n",
    "## 모델링\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model= RandomForestClassifier(n_estimators= 10) # help(model) #fit # 메모리 줄이기. n_estimators=100 default. 10으로 해결.\n",
    "\n",
    "# display(x_train.head())\n",
    "# print(x_train.shape, x_test.shape) # 둘다 (3500,10). x_test 가 왜 3500? 원래 (2482,10) 인데\n",
    "# print(x_train.columns, x_test.columns) # 컬럼 일치\n",
    "# print(\"y_train.shape: \", y_train.shape) #(3500, 2) . 정상적으로 2컬럼.\n",
    "# print(\"y_train.iloc[:,1]: \", y_train.iloc[:,1])\n",
    "\n",
    "\n",
    "## 메모리 이슈 -  여분이 7.9GB 인데 랜포 돌리면 8.5GB 를 드신다. PC에서 돌리면 돌아갈까? 랜포말고 DecisionTree는?\n",
    "# n_estimators=10 으로 해결. # n_estimators=40 에서 재발- MemoryError: could not allocate 458752000 bytes. 437MB\n",
    "\n",
    "## train 에 index 넣는 게 좀 문제 있어 보여서 cust_id 그냥 drop 시키고 진행\n",
    "## predict 는 슬라이싱 하지 말아야 하고\n",
    "# id 가 꼬이게 된다. model.fit(x_train, y_train) #display(pd.DataFrame(y_train_predict).head()) # <<< 2열 (인덱스,예측 결과값)\n",
    "model.fit(x_train, y_train.iloc[:,1]) # y_test_predict 용 모델도 gender 만 넣고\n",
    "y_test_predict= model.predict(x_test) \n",
    "\n",
    "## proba 는 슬라이싱 해서 gender 만 내도록 해야 한다.\n",
    "model.fit(x_train, y_train.iloc[:,1]) # <<< 없는 cust_id 가 방해가 되나 싶어서 1열만 내도록 학습\n",
    "y_test_proba= model.predict_proba(x_test) # <<< train 으로 만든 모델에 x_test 넣기. 결과 제출용으로 필수\n",
    "\n",
    "# y_train_predict= model.predict(x_train) # <<< score 는 보류\n",
    "\n",
    "# display(pd.DataFrame(y_test_predict).head())\n",
    "## help(pd.concat) # pd.concat([series1, series2], ignore_index=True, axis=1)\n",
    "display(pd.concat([x_test_custid, pd.Series(y_test_predict)], ignore_index=True, axis=1)) # y_test_predict 는 ndarray\n",
    "\n",
    "## id 가 포함된 채로 y_train 목표로 학습(fit)시킬 경우, y_test_predict 의 id 에는 중복값이 들어가있다.\n",
    "# display(pd.DataFrame(y_test_predict).sort_values(0, ascending=True).head())\n",
    "\n",
    "# print(f\"len(y_test_predict) {len(y_test_predict)}\")\n",
    "# print(f\"y_test_predict {y_test_predict}\")\n",
    "print(f\"y_test_proba {y_test_proba} \\ny_test_proba[0] {y_test_proba[0]}\")\n",
    "# print(f\"len(y_test_proba) {len(y_test_proba)} \\nlen(y_test_proba[0]) {len(y_test_proba[0])}\")\n",
    "# print(f\"y_test_proba {y_test_proba} \\ny_test_proba[0] {y_test_proba[0]} \\ny_test_proba[0][0] {y_test_proba[0][0]}\")\n",
    "\n",
    "## y_train 에 index가 있으면 빼고 학습시켜야 proba가 정상적으로 나온다\n",
    "# 해결! ValueError: could not broadcast input array from shape (3500,3500) into shape (3500,)\n",
    "display(pd.DataFrame(y_test_proba).head()) # <<< columns=[0,1], 첫 행= [0.5, 0.5]\n",
    "display(pd.DataFrame(y_test_proba).info())\n",
    "\n",
    "# print(f\"y_test_proba {y_test_proba} \\ny_test_proba[0] {y_test_proba[0]} \\ny_test_proba[0][0]\\\n",
    "#     {y_test_proba[0][0]}\")\n",
    "\n",
    "# print(f\"len(y_test_proba) {len(y_test_proba)} \\nlen(y_test_proba[0]) {len(y_test_proba[0])}  \\\n",
    "#     \\nlen(y_test_proba[0][0]) {len(y_test_proba[0][0])}\")\n",
    "\n",
    "# # help(pd.DataFrame) # example. df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
    "# print(pd.DataFrame(data=y_test_proba, columns=['custid', 'gender'])) \n",
    "\n",
    "\n",
    "## score 는 필수사항은 아니므로 일단 보류\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# y_test 는 애초에 없다. print(roc_auc_score(y_test, y_test_predict))\n",
    "#ValueError: multiclass-multioutput format is not supported. 보류\n",
    "# 내가 만든 모델(y_train_predict)의 점수만 확인 가능. \n",
    "# print(roc_auc_score(y_train, pd.DataFrame(data=y_train_predict, columns=['cust_id', 'gender'])) )\n",
    "\n",
    "\n",
    "# afterModeling= time.time()\n",
    "# print(f\"모델링 시간: {afterModeling-beforeModeling:.6} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4f939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de35f669",
   "metadata": {},
   "source": [
    "### 각종 시행착오 및 주석 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e58dd4",
   "metadata": {},
   "source": [
    "1. loc에 컬럼 여럿 print(x_train.describe().loc[:,('총구매액', '환불금액')])\n",
    "2. print 대신 display 로 보면 값-컬럼 정렬되있어서 보기 좋다\n",
    "3. [column rename python](https://rfriend.tistory.com/468) 특정 컬럼만 콕 집어 바꿀 수 있다.  df.rename(columns = {\"old\": \"new\"}, inplace = True)\n",
    "4. 메모리 부족 에러: 코랩이든 남의 컴이든 똑같을 것. 변수 개수 줄이고 각종 중간 변수들 생략하고, 에러나도 메모리 해제하게끔 조치해보자. \n",
    "> n_estimators=100 대신 40 넣고, OS 상의 다른 서비스 끄고, 변수 줄여서 성공. 파이썬에 메모리 해제하는 코드는 없대\n",
    "4. 랜덤포레스트 에서 model.score() 시, \"ValueError: X has 2 features, but DecisionTreeClassifier is expecting 10 features as input\"\n",
    "> 랜포에 왠 Decision.. 일단 따지지 않고 model.score() 안 쓰고 sklearn.metrics: roc_auc_score 쓰는걸로 임시방편\n",
    "5. [ValueError: could not convert string to float: '기타'](https://hashcode.co.kr/questions/9650/valueerror-could-not-convert-string-to-float-%EB%AC%B8%EC%9E%90-%EB%9D%BC%EB%8A%94-%EC%97%90%EB%9F%AC). 기타 컬럼의 string 에 숫자 연산이 필요한 무언가를 했을 것\n",
    "> x_train, y_train, x_test 에 모두 '기타' 컬럼이 존재하지 않는다. 아마 LabelEncoding 이 필요한 '주구매상품, 주구매지점' 에 연산을 하다가 가장 처음 만난 요소가 '주구매상품:기타' 일 것.\n",
    "- x_test 에도 x_train 에 했던 것을 똑같이 해줘야 한다는 건데.. x_validation 넣으면 또 안될거 아녀\n",
    "> 시험 혹은 채점자의 검증은 y_test 와 일치여부만을 본다. x_validation 이 만약에 있다면 그 또한 전처리해서 모델러가 사용할 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff0e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?????? '기타' 라는 컬럼이 없는데 어디서 나온 컬럼이여-\n",
    "# print(x_train.columns, x_test.columns, y_train.columns)\n",
    "# data= pd.read_csv('bigData/x_train.csv', encoding='CP949')\n",
    "# data['주구매상품'].value_counts() # 여기에 '기타' 있다. 가장 많은 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cafad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델링전까지 수행시간: 0.151001 sec\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '기타'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13168/3678896618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[0my_train_predict\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# <<<\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m \u001b[0my_test_predict\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# <<< train 으로 만든 모델에 x_test 넣기. roc_auc_score 뽑으려면 필수.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;31m# 당연히 에러나겠지. 파생변수 없고 null 이 그득해서.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[0;32m    408\u001b[0m                                     reset=False)\n\u001b[0;32m    409\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '기타'"
     ]
    }
   ],
   "source": [
    "## 데이터 로드, null, dtypes 체크 #이상치 체크, 인코딩, 파생변수 # 스케일링 # 모델링, train score, test score # 형식에 맞게 출력\n",
    "## gender 일 확률. proba\n",
    "\n",
    "\n",
    "# !ls\n",
    "import time\n",
    "start_time= time.time()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "# help(pd.read_csv) # encoding #UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 8: invalid continuation byte\n",
    "x_train= pd.read_csv('bigData/x_train.csv', encoding='CP949')\n",
    "y_train= pd.read_csv('bigData/y_train.csv', encoding='CP949')\n",
    "x_test= pd.read_csv('bigData/x_train.csv', encoding='CP949')\n",
    "# display(type(x_train))\n",
    "# display(x_train.shape, x_train.info(). x_train.head(2)) # AttributeError: 'NoneType' object has no attribute 'x_train'\n",
    "# info() 뒤에 , 대신 . 을 찍었다\n",
    "# display(x_train.shape, x_train.info(), x_train.head(2)) # 총 3500개. 환불금액 1205개. 주구매상품 주구매지점 object\n",
    "# display(y_train.shape, y_train.info(), y_train.head(2)) # cust_id, gender. 제출형식에는 _ 없음에 유의\n",
    "# display(x_test.shape, x_test.info(), x_test.head(2)) # 총 3500개. train-test 가 반반이네.\n",
    "\n",
    "## null 처리. 환불 null 은 환불이 없었다고 가정. fillna(0)\n",
    "# display(x_train['환불금액'].min() ,'---') # min: 5600. null 이 아닌 것 중 0인 것은 없다\n",
    "# 틀림 x_train.loc[x_train.isnull().sum()!=0]\n",
    "# display(x_train.loc[x_train.isnull().sum(1) !=0],'---')\n",
    "# display(x_train.describe().loc[:,('총구매액', '환불금액')],'---')\n",
    "# 환불금액이 null 인 것의 의미? 가정1. 적지 않았다. 가정2. 환불이 없었다. - 2295 개나 적지 않았다는걸 받아들이기 어려움\n",
    "# 환불금액을 mean 값으로 대체할 경우 문제점: 총구매액보다 커질 수 있다.\n",
    "x_train['환불금액']= x_train['환불금액'].fillna(0)\n",
    "# display(x_train.loc[x_train.isnull().sum(1)!=0].describe(),'---')\n",
    "# display(y_train.loc[x_train.isnull().sum(1)!=0],'---') # y_train 에는 null 이 없다\n",
    "\n",
    "## dtypes 체크 - 숫자인데 문자로 되어있는 건 아니 보인다\n",
    "# display(x_train.columns[x_train.dtypes==object])\n",
    "# display(x_train.loc[:,x_train.dtypes==object].iloc[:,0].value_counts().count()) # 42종 카테고리\n",
    "# display(x_train.loc[:,x_train.dtypes==object].iloc[:,1].value_counts().count()) # 24종 카테고리\n",
    "# display(x_train.columns[x_train.dtypes!=object])\n",
    "# display(x_train.loc[:,x_train.dtypes!=object])\n",
    "# display(x_train.describe())\n",
    "\n",
    "## 이상치 보기. 구매액이 매우 크다고 해서 절삭하는 건 말이 안 된다고 본다.\n",
    "# 대신 boundary 바깥의 값을 파생변수에 이용할 수도?\n",
    "def outlierCheck(data):\n",
    "    dataNum= data.loc[:,data.dtypes!=object] # 숫자만\n",
    "    \n",
    "    desc= dataNum.describe()\n",
    "    min1= desc.loc['min']\n",
    "    max1= desc.loc['max']\n",
    "    std= desc.loc['std']\n",
    "    mean= desc.loc['mean']\n",
    "    maxBoundary= mean+1.5*std\n",
    "    minBoundary= mean-1.5*std\n",
    "#     print(\"maxBoundary\", maxBoundary, '---')\n",
    "#     print(\"minBoundary\", minBoundary, '---')\n",
    "    \n",
    "#     print((dataNum>maxBoundary).sum(0), '---') # 큰 이상치 수백개\n",
    "#     print((dataNum<minBoundary).sum(0), '---') # 작은 이상치 없음\n",
    "    \n",
    "    return minBoundary, maxBoundary\n",
    "\n",
    "minB, maxB= outlierCheck(x_train)\n",
    "\n",
    "## 파생변수. 주말방문 경험 유무? 1회 최대구매액 구간- boundary 밖 & median? 환불여부?\n",
    "# 주말방문여부, 환불여부, 최대구매액많은편\n",
    "# display(x_train.head(3))\n",
    "conditionWeekend= x_train['주말방문비율']>0\n",
    "x_train.loc[conditionWeekend, '주말방문여부']= 1\n",
    "x_train.loc[~conditionWeekend, '주말방문여부']= 0\n",
    "# display(x_train['내점일수'].value_counts(), '---')\n",
    "# display(x_train['구매주기'].count(), '---')\n",
    "# display(x_train[x_train['구매주기']<=4].count(), '---') # 기준값 모르겠다\n",
    "# display(x_train.describe().loc[:,('총구매액','최대구매액','환불금액')], '---')\n",
    "conditionRefund= x_train['환불금액']>0\n",
    "x_train.loc[conditionRefund, '환불여부']= 1\n",
    "x_train.loc[~conditionRefund, '환불여부']= 0\n",
    "# print(maxB)\n",
    "conditionMaxBuyOutlier= x_train['최대구매액']> maxB['최대구매액']\n",
    "conditionMaxBuy= (x_train['최대구매액']<= maxB['최대구매액']) & (x_train['최대구매액'] > x_train['최대구매액'].median())\n",
    "conditionMaxBuySmall= x_train['최대구매액'] <= x_train['최대구매액'].median()\n",
    "x_train.loc[conditionMaxBuyOutlier, '최대구매액많은편']=2\n",
    "x_train.loc[conditionMaxBuy, '최대구매액많은편']=1\n",
    "x_train.loc[conditionMaxBuySmall, '최대구매액많은편']=0\n",
    "\n",
    "# display(x_train.head(), '---')\n",
    "# display(x_train['최대구매액많은편'].value_counts()) # 2는 207개\n",
    "\n",
    "## 메모리 관리. 변수 지우기\n",
    "x_train.drop(columns='환불금액', inplace=True)\n",
    "\n",
    "## 책- 상관관계\n",
    "# display((x_train.corr() < -0.7) |(x_train.corr()>0.7))\n",
    "# => (총구매액~최대구매액) , (최대구매액많은편~최대구매액) 의 상관관계가 0.7 초과이므로, 최대구매액을 없애자\n",
    "x_train.drop(columns='최대구매액', inplace=True)\n",
    "# display((x_train.corr() < -0.7) |(x_train.corr()>0.7)) # 더이상 다중공선성 문제있는 것은 없다\n",
    "# display(x_train.head(), '---')\n",
    "\n",
    "## 인코딩. 범주화\n",
    "# display(x_train.columns[x_train.dtypes==object], '---') # ['주구매상품', '주구매지점']\n",
    "# import sklearn # help(sklearn)\n",
    "# from sklearn import preprocessing # dir(preprocessing) # LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# help(LabelEncoder) # examples #fit_transform(self, y)\n",
    "encoder= LabelEncoder()\n",
    "# print(type(encoder.fit_transform(x_train['주구매상품']))) # ndarray. inplace 없다\n",
    "x_train['주구매상품']= encoder.fit_transform(x_train['주구매상품']) # astype 바로 못 붙인다\n",
    "x_train['주구매지점']= encoder.fit_transform(x_train['주구매지점'])\n",
    "x_train['주구매상품']= x_train['주구매상품'].astype('category') # inplace 없다\n",
    "x_train['주구매지점']= x_train['주구매지점'].astype('category')\n",
    "# 파생변수 범주화\n",
    "x_train['주말방문여부']= x_train['주말방문여부'].astype('category')\n",
    "x_train['환불여부']= x_train['환불여부'].astype('category')\n",
    "x_train['최대구매액많은편']= x_train['최대구매액많은편'].astype('category')\n",
    "\n",
    "\n",
    "# display(x_train.dtypes, x_train.head(), '---')\n",
    "\n",
    "## cust_id 를 custid 로 변경하고 index 로 만들기\n",
    "# print(y_train.head())\n",
    "# y_train 은 cust_id 로 되어 있어서, 미리 변경하지 말고 to_csv 하기 직전에 바꾸자 <<\n",
    "## 나중에 되돌리는 건 df.reset_index(drop=False, inplace=True)\n",
    "# y_train 으로 보내야 하니 버릴 수는 없고, \n",
    "# help(x_train) # rename( . df.rename(columns={\"A\": \"a\", \"B\": \"c\"}, inplace=Ture)\n",
    "# x_train.rename(columns={\"cust_id\":\"custid\"}, inplace=True) # 콜론!\n",
    "# print(x_train.columns[0])\n",
    "# print(x_train.columns[1::])\n",
    "# print(x_train.columns, '---')\n",
    "x_train.index= x_train['cust_id']\n",
    "x_train.drop(columns='cust_id', inplace=True)\n",
    "\n",
    "# display(x_train.head(), x_train.dtypes, '---')\n",
    "\n",
    "\n",
    "## 스케일링 - 시험때는 조건말고 눈으로 보고 컬럼 골라내서 for 문 돌리자\n",
    "# + 조건으로 거니까 앞에 코드에서 변경이 있어도, 최소 변경만으로 수월하게 진행가능\n",
    "# from sklearn import preprocessing # dir(preprocessing) # gender는 범주형이니, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "x_trainNumCols= x_train.columns[(x_train.dtypes!=object) &(x_train.dtypes!='category')]\n",
    "# print(x_trainNumCols)\n",
    "# print(x_trainNumCols[x_train.describe().loc['max']> 10000])\n",
    "bigNumCols= x_trainNumCols[x_train.describe().loc['max']> 30] # 30 근거: 범주컬럼 냅두고 싶어서 눈으로 정한 값\n",
    "# scaler.fit_transform()\n",
    "for col in bigNumCols:\n",
    "    x_train[col]= scaler.fit_transform(x_train[[col]])\n",
    "    \n",
    "# display(x_train.head(), x_train.dtypes, '---')\n",
    "\n",
    "beforeModeling= time.time()\n",
    "print(f\"모델링전까지 수행시간: {beforeModeling-start_time:.6} sec\")\n",
    "\n",
    "## 모델링\n",
    "# import sklearn # help(sklearn) #package. ensemble\n",
    "# from sklearn import ensemble # dir(ensemble) #RandomForestClassifier - gender\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model= RandomForestClassifier(n_estimators= 40) # help(model) #fit # 메모리 줄이기. n_estimators=100 default\n",
    "# print(y_train.head()) # y_train 이 확률값이 아니다 ! 모델은 값으로 만들고 나중에 제출할 때만 _proba 해야!\n",
    "\n",
    "## 메모리 이슈\n",
    "# 에러! MemoryError: could not allocate 458752000 byte= 437MB. 여유분 약 3GB\n",
    "# MemoryError: could not allocate 229376000 bytes= 218MB\n",
    "# model.fit_transform(x_train, y_train) # RandomForestClassifier 에 fit_transform 이 없다.\n",
    "\n",
    "model.fit(x_train, y_train) # <<<\n",
    "\n",
    "# 틀림. proba는 제출할때. y_train_proba= model.predict_proba(x_train) \n",
    "# 여태 만진 게 train인데 왜 갑자기 test를. 굳이 train ㄴㄴ. 바로 test ㄱㄱ. y_train_predict= model.predict(x_train)\n",
    "# 에러 y_test_predict= model.predict(x_test) # ValueError: could not convert string to float: '기타'\n",
    "\n",
    "y_train_predict= model.predict(x_train) # <<<\n",
    "y_test_predict= model.predict(x_test) # <<< train 으로 만든 모델에 x_test 넣기. roc_auc_score 뽑으려면 필수.\n",
    "# 당연히 에러나겠지. 파생변수 없고 null 이 그득해서.\n",
    "\n",
    "display(pd.DataFrame(y_train_predict).head()) # <<<\n",
    "\n",
    "# print(model.score(y_train, y_train_predict)) # <<<\n",
    "# Error: ValueError: X has 2 features, but DecisionTreeClassifier is expecting 10 features as input\n",
    "# 랜포에 왠 의사결정나무.?? 따지지 말고 model.score 를 쓰지 말자.\n",
    "\n",
    "# from sklearn import metrics # dir(metrics) # roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# help(roc_auc_score) #roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "print(roc_auc_score(y_test, y_test_predict))\n",
    "\n",
    "\n",
    "afterModeling= time.time()\n",
    "print(f\"모델링 시간: {afterModeling-beforeModeling:.6} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf42a02",
   "metadata": {},
   "source": [
    "[Memory Error 해결방법](https://bskyvision.com/799)\n",
    "114688000 byte(109 MB) 까지 줄였다\n",
    "1. 재실행 및 재부팅 후 실행 -> 마찬가지\n",
    "2. batch 사이즈 줄이기 -> n_estimators, max_depth 정도?\n",
    "> n_estimators=40\n",
    "3. 도중에 중단되도 메모리 해제하기 -> 파이썬에는 메모리 관리 명령어가 없다 (!!)\n",
    "> [python try except finally memory leak](https://stackoverflow.com/a/60454634)\n",
    "4. 변수를 줄인다 -> 파생변수를 지우거나 원본변수를 지우거나\n",
    "> 최대구매금액, 환불금액 삭제\n",
    "5. 내 PC에서 메모리를 먹고 있는 다른 서비스 종료\n",
    "6. 페이징 파일을 증가시킨다 -> 내 PC에서는 가능하더라도, 시험장에서는 .?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2819413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# help(RandomForestClassifier) \n",
    "#(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "# min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "#n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, \n",
    "# max_samples=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
